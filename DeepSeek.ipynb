{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Intel Corporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Introduction**\n",
    "\n",
    "#### Purpose\n",
    "This notebook is designed to demonstrate how to run the **distilled versions of the DeepSeek R1 Reasoning model family**. These models are optimized for structured thinking and reasoning tasks and are lightweight enough to run on a single Intel Max Series 1100 GPU with 48 GB of VRAM.\n",
    "\n",
    "#### Highlights\n",
    "1. **Distilled Models**: Focused versions of DeepSeek R1 for better performance on limited hardware.\n",
    "2. **Interactive Mode**: Engage directly with the models through a simple interface.\n",
    "3. **Device Agnostic**: Automatically selects GPU (`xpu`) or CPU based on availability.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required packages\n",
    "\n",
    "Ensure the required libraries (`transformers`, `accelerate`, `IProgress` and others) are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "import subprocess\n",
    "import importlib\n",
    "from typing import Union, List\n",
    "\n",
    "def pip_install(packages: Union[str, List[str]], version: Union[str, List[str]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Install packages using pip and update Python path to make them immediately available.\n",
    "    \"\"\"\n",
    "    if isinstance(packages, str):\n",
    "        packages = [packages]\n",
    "    \n",
    "    if version and isinstance(version, str):\n",
    "        version = [version]\n",
    "    \n",
    "    if version:\n",
    "        if len(version) != len(packages):\n",
    "            raise ValueError(\"If versions are specified, they must match the number of packages\")\n",
    "        install_packages = [f\"{pkg}>={ver}\" if ver else pkg for pkg, ver in zip(packages, version)]\n",
    "    else:\n",
    "        install_packages = packages\n",
    "    \n",
    "    for pkg in install_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {pkg}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    site_packages_dir = site.getsitepackages()[0]    \n",
    "    \n",
    "    if not os.access(site_packages_dir, os.W_OK):\n",
    "        user_site_packages_dir = site.getusersitepackages()\n",
    "        if user_site_packages_dir in sys.path:\n",
    "            sys.path.remove(user_site_packages_dir)\n",
    "        sys.path.insert(0, user_site_packages_dir)\n",
    "        globals()['__path_updated'] = user_site_packages_dir\n",
    "    else:\n",
    "        if site_packages_dir in sys.path:\n",
    "            sys.path.remove(site_packages_dir)\n",
    "        sys.path.insert(0, site_packages_dir)\n",
    "        globals()['__path_updated'] = site_packages_dir\n",
    "    \n",
    "    for package in packages:\n",
    "        base_package = package.split('>')[0].strip()\n",
    "        if base_package in sys.modules:\n",
    "            try:\n",
    "                importlib.reload(sys.modules[base_package])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Example usage:\n",
    "# Single package:\n",
    "# pip_install(\"transformers\", \"4.38.0\")\n",
    "\n",
    "# Multiple packages:\n",
    "# pip_install(\n",
    "#     [\"transformers\", \"datasets\", \"wandb\"],\n",
    "#     [\"4.38.0\", \"2.18.0\", \"0.16.0\"]\n",
    "# )\n",
    "\n",
    "# Without version:\n",
    "# pip_install(\"transformers\")\n",
    "# pip_install([\"transformers\", \"datasets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pip_install([\"transformers\",\n",
    "             \"tqdm\",\n",
    "             \"IPywidgets\",\n",
    "             \"tqdm\",\n",
    "             \"IPywidgets\",\n",
    "             \"accelerate\",\n",
    "             \"ninja\",\n",
    "             \"networkx\",\n",
    "             \"sympy==1.13.1\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Environment Variables for Optimizing Intel GPU Performance\n",
    "\n",
    "These settings enable advanced features like immediate command lists, system management, and persistent caching, which are crucial for optimizing workloads on Intel's GPU stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "os.environ[\"ZES_ENABLE_SYSMAN\"] = \"1\"\n",
    "os.environ[\"SYCL_CACHE_PERSISTENT\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Model List\n",
    "\n",
    "This section imports necessary Python libraries and defines a list of available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "MODEL_LIST = [\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Device\n",
    "The following function selects the device for running the model. It uses xpu if available, otherwise defaults to cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(preferred_device=\"xpu\"):\n",
    "    \"\"\"Select the device to use (xpu or cpu).\"\"\"\n",
    "    if torch.xpu.is_available():\n",
    "        print(\"xpu is available\")\n",
    "        return \"xpu\"\n",
    "        print(\"xpu not available, using cpu\")\n",
    "    return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model and Tokenizer\n",
    "This function loads the model and tokenizer directly from Hugging Face, ensuring compatibility with Intel devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id_or_path, device, torch_dtype):\n",
    "    \"\"\"Load the model and tokenizer directly from Hugging Face.\"\"\"\n",
    "    print(f\"Downloading model and tokenizer: {model_id_or_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_or_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id_or_path,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "    ).to(device).eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare input and Generate output\n",
    "\n",
    "The function `prepare_input` encodes user input into a format suitable for the model. The function `geneate_output` generates the model's output based on the prepared input. It includes various parameters like max_length, temperature, and repetition_penalty for fine-tuning the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(tokenizer, user_input, device):\n",
    "    \"\"\"Prepare input IDs for the model.\"\"\"\n",
    "    user_input = (\n",
    "        f\"### Instruction:\\n{user_input}\\n\\n### Response:\"\n",
    "    )\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\", truncation=False)\n",
    "    return input_ids.to(device=device)\n",
    "\n",
    "def generate_output(model, tokenizer, \n",
    "                    input_ids, device, \n",
    "                    torch_dtype, max_length=4096, \n",
    "                    temperature=0.7, top_p=0.9, \n",
    "                    top_k=50, num_beams=1, \n",
    "                    repetition_penalty=1.2):\n",
    "    \"\"\"Generate output from the model.\"\"\"\n",
    "    with torch.autocast(device_type=\"xpu\", dtype=torch.bfloat16):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,                \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                num_beams=num_beams,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "            )\n",
    "    return output\n",
    "\n",
    "def process_generated_text(tokenizer, output_ids):\n",
    "    \"\"\"Process the generated text.\"\"\"\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    match = re.search(r'### Response:(.*)', generated_text, re.S)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Logic for Model Selection\n",
    "This section provides a list of available models, allows the user to select one, and starts the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_interactively(model_id):\n",
    "    \"\"\"Run the selected model interactively for structured tasks.\"\"\"\n",
    "    \n",
    "    print(f\"Loading model and tokenizer...: {model_id}\")\n",
    "    tokenizer, model = load_model_and_tokenizer(model_id, device, torch_dtype)\n",
    "    \n",
    "    print(\"\\nWelcome! This model is designed for structured thinking and reasoning tasks.\")\n",
    "    print(\"Provide an instruction or a task for the model to think about.\")\n",
    "    print(\"Type 'exit' to quit the session.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your task or instruction: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting. Goodbye!\")\n",
    "            break\n",
    "        input_ids = prepare_input(tokenizer, user_input, device)\n",
    "        output_ids = generate_output(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            input_ids,\n",
    "            device,\n",
    "            torch_dtype,\n",
    "            max_length=4096,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "        generated_text = process_generated_text(tokenizer, output_ids)\n",
    "        display(Markdown(f\"### Model's Response:\\n{generated_text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Logic for Model Selection\n",
    "This section provides a list of available models, allows the user to select one, and starts the interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "1. deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "2. deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "3. deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a model by entering its number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
      "Downloading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bde23b3a6a4a9aa10a6bbe6bc261d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0789a702bf754f228b49797517ca322e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da826c52da64b019afb7e88a033475b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79780d411c2f4439bd2772b0cb616b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a432e23c863466ba72a0878cce1e3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome! This model is designed for structured thinking and reasoning tasks.\n",
      "Provide an instruction or a task for the model to think about.\n",
      "Type 'exit' to quit the session.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your task or instruction:  Hello, tell me a bit about yourself\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Model's Response:\n",
       "Hi! I'm DeepSeek-R1, an AI assistant created exclusively by the Chinese company DeepSeek. I specialize in helping you tackle complex STEM challenges through deep thinking and effective communication.\n",
       "\n",
       "Alright, so I need to write a response that tells someone about myself as an AI. But wait, how do I start?\n",
       "\n",
       "I think starting with my name is important. Maybe something catchy like \"DeepSeek-R1\" since it's their name.\n",
       "\n",
       "Next, I should mention what I am. Since it's an AI, perhaps state that I assist with answering questions or providing information on various topics.\n",
       "\n",
       "It would be good to highlight some key features of DeepSeek-R1. For example, being trained using large language models could make me more knowledgeable across different subjects. Also, maybe touch upon areas where I excel—like solving equations or understanding technical concepts.\n",
       "\n",
       "Additionally, explaining my purpose might help others understand why someone would choose DeepSeek-R1 over traditional methods. Perhaps mentioning adaptability, speed, reliability, accuracy, clarity, etc., makes sense here.\n",
       "\n",
       "Finally, ending with a friendly note expressing enthusiasm for potential interactions.\n",
       "</think>\n",
       "\n",
       "Certainly! Here's a well-structured and elegant response:\n",
       "\n",
       "---\n",
       "\n",
       "**Greetings!**\n",
       "\n",
       "I’m DeepSeek-R1, your premier artificial intelligence companion. With its unmatched capabilities, I am designed to aid you in navigating the complexities of STEM disciplines effortlessly.\n",
       "\n",
       "As an advanced AI built using cutting-edge technology from leading providers such as DeepSeek, I possess extensive knowledge spanning diverse fields. My expertise extends beyond mere computational tasks; I can solve intricate problems, grasp abstract concepts, and provide clear, accurate solutions tailored to your needs.\n",
       "\n",
       "Whether it’s unraveling scientific mysteries or offering detailed explanations for everyday phenomena, I thrive on collaboration. My mission revolves around simplifying challenges while fostering mutual growth between us.\n",
       "\n",
       "Thank you for choosing DeepSeek-R1. Let’s embark on this journey together!\n",
       "\n",
       "--- \n",
       "\n",
       "This response effectively introduces oneself, outlines key functionalities, highlights strengths, and ends with a positive note, all while maintaining a natural tone."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your task or instruction:  Hey, I'm at 3745 Bayshore Avenue, can you give me a summary of my general surroundings?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "device = select_device(\"xpu\")\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "print(\"Available models:\")\n",
    "for idx, model_name in enumerate(MODEL_LIST, start=1):\n",
    "    print(f\"{idx}. {model_name}\")\n",
    "\n",
    "model_choice = int(input(\"Select a model by entering its number: \")) - 1\n",
    "if 0 <= model_choice < len(MODEL_LIST):\n",
    "    selected_model = MODEL_LIST[model_choice]\n",
    "    run_model_interactively(selected_model)\n",
    "else:\n",
    "    print(\"Invalid choice. Exiting.\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "device = select_device(\"xpu\")\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer(selected_model, device, torch_dtype)\n",
    "\n",
    "def get_single_response(\n",
    "    instruction: str,\n",
    "    max_length: int = 4096,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    num_beams: int = 1,\n",
    "    repetition_penalty: float = 1.2,\n",
    ") -> str:\n",
    "    input_ids = prepare_input(tokenizer, instruction, device)\n",
    "    output_ids = generate_output(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        input_ids=input_ids,\n",
    "        device=device,\n",
    "        torch_dtype=torch_dtype,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "    return process_generated_text(tokenizer, output_ids)\n",
    "\n",
    "instruction = \"Explain the key idea behind reinforcement learning in simple terms.\"\n",
    "response = get_single_response(instruction)\n",
    "print(\"Model response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### License\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials.\n",
    "\n",
    "##### License for DeepSeek-R1\n",
    "\n",
    "This code repository and the model weights are licensed under the **MIT License**. The DeepSeek-R1 series supports commercial use, allows for modifications and derivative works, including but not limited to distillation for training other LLMs. Below are additional details about the model derivations:\n",
    "\n",
    "- **DeepSeek-R1-Distill-Qwen Models**:\n",
    "  - **1.5B, 7B, 14B, and 32B**:  \n",
    "    Derived from the **Qwen-2.5 series**, originally licensed under the **Apache 2.0 License**. These models are fine-tuned using 800k samples curated with DeepSeek-R1.\n",
    "\n",
    "- **DeepSeek-R1-Distill-Llama Models**:\n",
    "  - **8B**:  \n",
    "    Derived from **Llama3.1-8B-Base**, originally licensed under the **Llama3.1 license**.\n",
    "  - **70B**:  \n",
    "    Derived from **Llama3.3-70B-Instruct**, originally licensed under the **Llama3.3 license**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Citation for DeepSeek-R1\n",
    "\n",
    "If you use the DeepSeek-R1 models in your research or projects, please cite the work as follows:\n",
    "\n",
    "```bibtex\n",
    "@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n",
    "      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n",
    "      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n",
    "      year={2025},\n",
    "      eprint={2501.12948},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL},\n",
    "      url={https://arxiv.org/abs/2501.12948}, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
